%! Author = maximecolliat
%! Date = 09/01/2025

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

\item \chapter*{Résumé}


\chapter*{Introduction}


\section*{2.1 Contexte}
Dans le cadre du module d’Informatique Décisionnelle du Master 2 BDIA, ce projet propose d’explorer et d’appliquer les concepts fondamentaux de la création et de l’exploitation d’un data warehouse, un élément clé dans les environnements décisionnels modernes. L’importance croissante des données massives a profondément transformé les processus de prise de décision des entreprises, les poussant à s’appuyer sur des analyses précises et des outils avancés pour rester compétitifs.

Dans ce contexte, ce projet simule des problématiques réelles rencontrées par des analystes, des décideurs ou des responsables d’entreprise souhaitant exploiter des données volumineuses pour identifier des tendances, formuler des prévisions ou optimiser leurs opérations. Les données proviennent de l’application Yelp, une plateforme reconnue qui regroupe des millions d’avis d’utilisateurs sur divers commerces. 

Ce projet couvre l’ensemble des étapes essentielles du cycle de vie d’un système décisionnel : modélisation, intégration, analyse et présentation des données. 


\section*{2.2 Objectifs}
L’objectif principal de ce projet est de construire un système décisionnel complet et fonctionnel à partir d’un jeu de données volumineux et hétérogène fourni par l’application Yelp. Cela implique :


\begin{enumerate}
\item La modélisation d’un data warehouse adapté aux besoins analytiques identifiés, en choisissant entre des approches méthodologiques reconnues (Kimball ou Inmon).


\end{enumerate}

\begin{enumerate}
\item La conception et la mise en œuvre d’un processus ETL pour intégrer des données provenant de multiples formats et sources, tout en assurant leur cohérence et leur disponibilité pour l’analyse.


\end{enumerate}

\begin{enumerate}
\item L’élaboration de requêtes analytiques pertinentes, permettant d'explorer les données et de répondre à des questions stratégiques, telles que l’identification des commerces les mieux évalués ou la segmentation des utilisateurs selon leurs préférences.


\end{enumerate}

\begin{enumerate}
\item La création de tableaux de bord interactifs et intuitifs, permettant de synthétiser les résultats et de les communiquer efficacement à un public non technique.


\end{enumerate}

Pour la conception de notre data warehouse, nommé “Argon” en référence à l’élément chimique réputé pour sa stabilité et sa résistance, nous avons visé une architecture robuste et performante. Ce nom symbolise un système capable de centraliser et protéger les données tout en offrant une grande efficacité pour les analyses. Inspirée des propriétés de l’argon, notre conception met l’accent sur la simplicité, l’optimisation des performances, et l’isolation des composants pour garantir flexibilité et résilience face aux évolutions des données. 


Nos analyses se concentrent sur les commerces, en mettant l'accent sur les facteurs clés qui influencent leur implantation et leur succès dans différents lieux, en exploitant les données géographiques, les avis utilisateurs et les caractéristiques spécifiques des établissements.


\begin{enumerate}
\item \chapter*{Analyse du sujet et des données}

\end{enumerate}

\section*{3.1 Description des données disponibles}
Les données exploitées dans ce projet proviennent de la plateforme Yelp et sont organisées sous différents formats et sources. Elles offrent une vue détaillée des commerces, des utilisateurs et de leurs interactions. Voici une présentation des principales sources de données :


\subsection*{3.1.1. Fichiers CSV et JSON}

Fichier CSV "tip" : Contient des avis courts et succincts donnés par les utilisateurs. Ces tips incluent des informations comme le contenu textuel, l’identifiant de l’utilisateur, la date et le commerce concerné.


Fichiers JSON :

\begin{itemize}
\item "checkin" : Liste des visites (checkins) effectuées dans les commerces. Ces données incluent des horodatages précis, permettant de suivre la fréquentation des établissements sur une période donnée.


\item "business" : Informations détaillées sur les commerces, telles que leur localisation, leurs catégories, leurs horaires d'ouverture et des attributs supplémentaires (présence d’un parking, options alimentaires, etc.).


\end{itemize}

\subsection*{3.1.2. Tables dans la base PostgreSQL}
La base de données PostgreSQL contient plusieurs tables liées aux utilisateurs et aux avis :

\begin{itemize}
\item Table "yelp.user" : Fournit des informations sur les utilisateurs, telles que leur identifiant unique et les statistiques associées à leur activité (nombre de contributions, réactions reçues, etc.).


\item Table "yelp.friend" : Décrit les relations d’amitié entre utilisateurs, permettant de modéliser les réseaux sociaux et leurs impacts potentiels sur les évaluations.


\item Table "yelp.elite" : Indique les années où les utilisateurs ont obtenu le statut d’élite.


\item Table "yelp.review" : Contient les évaluations des utilisateurs avec des notes, des commentaires textuels et des métadonnées supplémentaires, telles que la date et l’identifiant du commerce évalué.


\end{itemize}
Voici le diagramme relationnel de ce schéma de base de données :



\section*{3.2 Analyse des besoins}

Identification des questions clés à résoudre (exemples fournis dans le document).


\section*{3.3 Définition des objectifs analytiques}

Objectifs à atteindre et cas d’usage envisagés.


\begin{enumerate}
\item \chapter*{Conception du Data Warehouse}

\end{enumerate}

\section*{4.1 Approche méthodologique}
Kimball


\section*{4.2 Spécifications des schémas}

Schéma étoile, flocon ou constellation. Présentation des tables de faits et des dimensions.


\section*{4.3 Justifications des choix techniques

}
Arguments pour les schémas choisis en fonction des objectifs analytiques.


\begin{enumerate}
\item \chapter*{Intégration des données}

\end{enumerate}

\section*{5.1 Architecture de l’ETL}

Un ETL est un processus d’extraction, de transformation et de chargement des données dans un entrepôt de données. Il est essentiel pour garantir la qualité et la cohérence des données.

Voici le schéma général de notre processus ETL :


\subsection{5.1.1 Extraction des données}


Nous avons choisi d’utiliser le langage Python avec différentes librairies comme Pandas pour implémenter notre ETL. Nous avons opté pour cette solution en raison de sa simplicité et l'aisance que nous avons grâce aux séances de l'UE Machine Learning et Deep Learning, où nous avons utilisé Pandas pour manipuler des données dans des fichiers CSV et JSON. 

Via des scripts Python développés pour chaque fichier, nous avons extrait les données et les avons stockées dans une base de données SQLite.
Ces scripts sont disponibles dans le dossier "/script" du projet.


Le choix de SQLite s’est imposé pour plusieurs raisons. Tout d’abord, il s’agit d’une solution légère et intégrée qui ne nécessite aucune configuration complexe ni serveur dédié,
ce qui simplifie le déploiement et la maintenance. Ensuite, SQLite est entièrement autonome et stocke les données dans un seul fichier, ce qui facilite grandement le transport et le
partage de la base de données. Enfin, sa compatibilité avec la plupart des systèmes d’exploitation et son support natif dans Python en font une option pratique et efficace pour
gérer les données du projet.
Ici, SQLite est utilisé comme un entrepôt de données temporaire pour stocker les données brutes avant de les transformer et de les charger dans le data warehouse final.
Nous voulons que les données soient stockées dans un format tabulaire pour faciliter leur manipulation et leur analyse ultérieure. SQLite offre une solution simple et efficace pour
stocker des données structurées dans des tables relationnelles, ce qui en fait un choix idéal pour cette tâche.


Voici un schéma de l'extraction des données et le schéma relationnel qui en découle :

\subsection{5.1.2 Transformation des données}

La transformation des données est une étape cruciale pour nettoyer, normaliser et enrichir les données brutes.
Elle permet de les rendre cohérentes et exploitables pour l’analyse. Cette phase inclut des opérations telles que le filtrage, le tri, la jointure, l’agrégation, la déduplication, etc.
À travers des fichiers SQL, nous avons implémenté les requêtes nécessaires pour transformer les données brutes en un format directement exploitable pour l’analyse.

\subsection{5.1.3 Chargement des données}

La dernière étape de l’ETL consiste à charger les données transformées dans le data warehouse. Ces données sont stockées dans des tables spécifiques, prêtes à être
analysées par des outils de business intelligence ou des applications d’analyse de données.

Les données transformées sont chargées dans un SGBD Oracle, qui est un système de gestion de
base de données relationnelle très performant et robuste. Il offre des fonctionnalités avancées pour l’analyse de données et la gestion des requêtes complexes.


\section*{5.2 Processus d’intégration}

Le processus d’intégration consiste à combiner les données provenant de différentes sources et à les stocker dans un entrepôt de données centralisé.
Cela permet de disposer d’une vue unifiée et cohérente des informations, facilitant leur analyse et leur exploitation.


\section{5.3 Problèmes rencontrés et solutions apportées}

Dans cette section, nous décrivons les problèmes rencontrés lors de l’extraction et de la transformation des données, ainsi que les solutions mises en place pour les résoudre.

\subsection{5.3.1 Problèmes rencontrés lors de l'extraction des données}


\subsubsection{5.3.1.1 Fichier JSON}

Le problème principal que nous avons rencontré lors de l'extraction des données est la gestion des fichiers JSON.
En effet, ces fichiers sont structurés de manière complexe et imbriquée, ce qui rend leur traitement plus difficile que les fichiers CSV.
Pour résoudre ce problème, nous avons dû développer des scripts Python spécifiques pour extraire les données pertinentes et les stocker dans des tables avant de les intégrer dans la base de données finale.
Lors de cette phase, nous n'avons pas "perdu" de données, mais nous avons dû faire des choix sur les informations à extraire et à stocker en fonction de leur pertinence pour les analyses futures.

\subsubsection{5.3.1.2 Fichier CSV}

Le problème rencontré avec le fichier CSV etait sa non conformité avec le schéma de la base de données.
En effet, ce fichier qui contient 4 colonnes, contient des lignes qui ont plus de 4 colonnes.
Cela est dû à la présence de virgules dans les champs textuels qui n'ont pas été correctement échappées et donc qui ont été interprétées comme des séparateurs de colonnes.
On se retrouve donc avec des lignes que l'on doit volontairement ignorer car elles ne sont pas exploitables.
Lors de cette partie, nous avons perdu 5.23% des données.
Le nombre total de lignes du fichier CSV est de 1375578 et le nombre de lignes ignorées est de 58243 soit 1317335 lignes qui ont été insérées dans la base de données.
Ce nombre de lignes est suffisant pour effectuer des analyses pertinentes et significatives, par contraite de temps, nous avons choisi de ne pas traiter ces lignes érronées.


\subsection{5.3.2 Problèmes rencontrés lors de la transformation des données}

Lors de la transformation des données, notre problème principal à été de nettoyer et de normaliser les données brutes pour les rendre cohérentes et exploitables pour l’analyse.
Cela implique de comprendre nos données et leurs caractéristiques.
Pour cela, nous avons exploré nos données en utilisant des requêtes SQL sur notre base de données.

\subsubsection{5.3.2.1 Fichier CSV}

Pour le fichier "yelp_academic_dataset_tip.csv", nous avons détecté des valeurs érronées dans toutes les colonnes.

Voici les elements qui nous ont permis de détecter ces valeurs érronées :

\begin{itemize}
\item Colonnes "user\_id" et "business\_id" : Un user\_id ou un business\_id ne peuvent pas être vide, ils ont une taille de 22 caractères et ne contiennent aucun espace.
Pour filtrer les lignes incorrectes, nous avons utilisé la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(business_id) != 22
OR business_id GLOB '* *'
OR length(user_id) != 22
OR user_id GLOB '* *'
OR user_id IS NULL
OR business_id IS NULL;
\end{verbatim}

On supprime donc les lignes qui ne respectent pas ces conditions et on est sûr que les données restantes sont cohérentes.

\item Colonne "compliment_count" : Cette colonne doit contenir des entiers positifs entre 0 et 10. On supprime donc les lignes qui ne respectent pas ces conditions via la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE compliment_count GLOB '*[^0-9]*'
OR compliment_count IS NULL;
\end{verbatim}

\item Colonne "date" : Cette colonne doit contenir des dates au format DATETIME. On supprime donc les lignes qui ne respectent pas ces conditions avec la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(date) != 19
OR date NOT GLOB '????-??-?? ??:??:??'
OR date IS NULL;
\end{verbatim}


Voici le tableau qui résume les données avant et après le nettoyage :

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Etape \& Nombre de lignes \\& Pourcentage de lignes conservées \\
\hline
Fichier brut \& 1375578 \& 100\% \\
\hline
Insertion \& 1317335 \& 95,76592531\% \\
\hline
Nettoyage \& 1315972 \& 95,66683968\% \\
\hline
\end{tabular}
\end{center}

Nous trouvons que le pourcentage de lignes conservées entre le fichier brut et le fichier nettoyé reste suffisamment élevé pour effectuer des analyses pertinentes et significatives.


\subsubsection{5.3.2.2 Fichier JSON}

Pour les fichiers "yelp_academic_dataset_business.json" et "yelp_academic_dataset_checkin.json", nous n'avons pas trouvé de problèmes particuliers, les données étaient déjà bien structurées et cohérentes.


\begin{enumerate}
\item \chapter*{Analyse et interrogation des données}

\end{enumerate}


\begin{itemize}
\item 5.1 Construction des Data Marts


\item 5.2 Requêtes et agrégations
Requêtes SQL exécutées, cubes analytiques, et résultats obtenus.


\item 5.3 Analyse des performances
Mesure des temps d’exécution et suggestions d’amélioration.


\end{itemize}

\begin{enumerate}
\item \chapter*{Présentation des résultats}

\end{enumerate}

\begin{itemize}
\item 6.1 Tableaux de bord et visualisations
Utilisation de Metabase pour créer des dashboards et visualisations.


\item 6.2 Synthèse des résultats analytiques
Interprétation des résultats, insights pour les utilisateurs finaux.


\end{itemize}

\begin{enumerate}
\item \chapter*{Documentation technique}

\end{enumerate}
\begin{itemize}
\item 8.1 Instructions pour la compilation et l’exécution du code
Procédure détaillée.


\item 8.2 Liste des dépendances et configurations
Bibliothèques utilisées, versions des outils, etc.


\end{itemize}

\begin{enumerate}
\item \chapter*{Bilan}

\end{enumerate}

\begin{itemize}
\item Résumé des contributions principales du projet.


\item Perspectives pour des développements futurs.


\end{itemize}

\begin{enumerate}
\item \chapter*{Annexes}

\end{enumerate}
}