\usepackage{graphicx}

\section*{Résumé}

Ce rapport présente les différentes étapes que nous avons suivies pour concevoir et implémenter un data warehouse à partir de données provenant de la plateforme Yelp.
L'introdcution présente le contexte du projet et les objectifs visés.
L'analyse du sujet et des données décrit les données disponibles, les besoins des utilisateurs et les objectifs analytiques.


\section*{Introduction}

\subsection{Contexte}\label{subsec:contexte}
Dans le cadre du module d’Informatique Décisionnelle du Master 2 BDIA, ce projet propose d’explorer et d’appliquer les concepts fondamentaux de la création et de l’exploitation d’un data warehouse, un élément clé dans les environnements décisionnels modernes. L’importance croissante des données massives a profondément transformé les processus de prise de décision des entreprises, les poussant à s’appuyer sur des analyses précises et des outils avancés pour rester compétitifs.

Dans ce contexte, ce projet simule des problématiques réelles rencontrées par des analystes, des décideurs ou des responsables d’entreprise souhaitant exploiter des données volumineuses pour identifier des tendances, formuler des prévisions ou optimiser leurs opérations. Les données proviennent de l’application Yelp, une plateforme reconnue qui regroupe des millions d’avis d’utilisateurs sur divers commerces. 

Ce projet couvre l’ensemble des étapes essentielles du cycle de vie d’un système décisionnel : modélisation, intégration, analyse et présentation des données.


\subsection*{Objectifs}
L’objectif principal de ce projet est de construire un système décisionnel complet et fonctionnel à partir d’un jeu de données volumineux et hétérogène fourni par l’application Yelp. Cela implique :

\begin{enumerate}
\item La modélisation d’un data warehouse adapté aux besoins analytiques identifiés, en choisissant entre des approches méthodologiques reconnues (Kimball ou Inmon).
\item La conception et la mise en œuvre d’un processus ETL pour intégrer des données provenant de multiples formats et sources, tout en assurant leur cohérence et leur disponibilité pour l’analyse.
\item L’élaboration de requêtes analytiques pertinentes, permettant d'explorer les données et de répondre à des questions stratégiques, telles que l’identification des commerces les mieux évalués ou la segmentation des utilisateurs selon leurs préférences.
\item La création de tableaux de bord interactifs et intuitifs, permettant de synthétiser les résultats et de les communiquer efficacement à un public non technique.
\end{enumerate}

Pour la conception de notre data warehouse, nommé “Argon” en référence à l’élément chimique réputé pour sa stabilité et sa résistance, nous avons visé une architecture robuste et performante. Ce nom symbolise un système capable de centraliser et protéger les données tout en offrant une grande efficacité pour les analyses. Inspirée des propriétés de l’argon, notre conception met l’accent sur la simplicité, l’optimisation des performances, et l’isolation des composants pour garantir flexibilité et résilience face aux évolutions des données. 


Nos analyses se concentrent sur les commerces, en mettant l'accent sur les facteurs clés qui influencent leur implantation et leur succès dans différents lieux, en exploitant les données géographiques, les avis utilisateurs et les caractéristiques spécifiques des établissements.


\chapter*{Analyse du sujet et des données}


\subsection*{3.1 Description des données disponibles}
Les données exploitées dans ce projet proviennent de la plateforme Yelp et sont organisées sous différents formats et sources. Elles offrent une vue détaillée des commerces, des utilisateurs et de leurs interactions. Voici une présentation des principales sources de données :


\subsection*{3.1.1. Fichiers CSV et JSON}

Fichier CSV "tip" : Contient des avis courts et succincts donnés par les utilisateurs. Ces tips incluent des informations comme le contenu textuel, l’identifiant de l’utilisateur, la date et le commerce concerné.


Fichiers JSON :

\begin{itemize}
\item "checkin" : Liste des visites (checkins) effectuées dans les commerces. Ces données incluent des horodatages précis, permettant de suivre la fréquentation des établissements sur une période donnée.


\item "business" : Informations détaillées sur les commerces, telles que leur localisation, leurs catégories, leurs horaires d'ouverture et des attributs supplémentaires (présence d’un parking, options alimentaires, etc.).


\end{itemize}

\subsection*{3.1.2. Tables dans la base PostgreSQL}
La base de données PostgreSQL contient plusieurs tables liées aux utilisateurs et aux avis :

\begin{itemize}
\item Table "yelp.user" : Fournit des informations sur les utilisateurs, telles que leur identifiant unique et les statistiques associées à leur activité (nombre de contributions, réactions reçues, etc.).


\item Table "yelp.friend" : Décrit les relations d’amitié entre utilisateurs, permettant de modéliser les réseaux sociaux et leurs impacts potentiels sur les évaluations.


\item Table "yelp.elite" : Indique les années où les utilisateurs ont obtenu le statut d’élite.


\item Table "yelp.review" : Contient les évaluations des utilisateurs avec des notes, des commentaires textuels et des métadonnées supplémentaires, telles que la date et l’identifiant du commerce évalué.


\end{itemize}

Voici le diagramme relationnel de ce schéma de base de données :



\subsection*{3.1.3. Élargissement des données}

Pour enrichir les analyses et les visualisations, nous avons envisagé d’intégrer des données supplémentaires, telles que des données démographiques, comme le nombre d'habitants par ville.
Ces données ont été obtenues à partir de sources externes (via API).
Nous pensons que ces informations complémentaires permettront de mieux comprendre les tendances et les caractéristiques des commerces, en les croisant avec des données géographiques et démographiques.


\section*{3.2 Analyse des besoins}

Pour répondre aux besoins des utilisateurs finaux, nous avons identifié plusieurs cas d’usage et scénarios d’analyse qui nécessitent des requêtes complexes et des agrégations de données.
Voici quelques exemples de questions auxquelles nous souhaitons répondre organisé par thème :

\begin{itemize}
\item Analyse de la géographie :
\begin{itemize}
\item Quels sont les 20 villes avec le plus de commerces ?
\item Quels sont les villes qui donnent en moyenne le plus de compliments ?
\item Quels sont les Etats qui font en moyenne le plus de checkins ?
\end{itemize}
\item Analyse des commerces :
\begin{itemize}
\item Quels sont les 200 business avec le plus de compliments
\end{itemize}
\item Analyse des horaires :
\begin{itemize}
\item Quels sont les heures d'ouverture les plus communes sur une semaine ?
\item Quels sont les heures d'ouverture les plus communes par jour de la semaine ?
\item Combien de business sont ouverts 24h/24 7j/7 ?
\end{itemize}
\item Analyse des attributs :
\begin{itemize}
\item Quels sont les attributs les plus courants des commerces ?
\item Quels sont les catégories les plus communes par ville ?
\item Quels sont les catégories les plus communes à Las Vegas ?
\item Quels sont les catégories qui plaisent le plus ?
\end{itemize}
\item Analyse des tendances :
\begin{itemize}
\item L'historique des checkins sur toute la période ?
\item Quels sont les jours de l'année avec le plus de checkins ?
\item Quels sont les jours de l'année avec le plus de tips ?
\end{itemize}
\end{itemize}

Ces questions nous permettront de mieux comprendre les tendances et les caractéristiques des commerces, ainsi que les préférences des utilisateurs.
Elles guideront la conception de notre data warehouse et l’élaboration des requêtes analytiques.


\section*{3.3 Définition des objectifs analytiques}

Sur la base des besoins identifiés, nous avons défini les objectifs analytiques suivants :

\begin{itemize}
\item Identifier les villes les plus attractives pour les commerces, en fonction de leur popularité et de leur nombre d'habitants.
\item Comprendre quels sont les commerces les mieux évalués, en analysant les avis et les notes des utilisateurs.
\item Analyser les horaires d’ouverture les plus courants des commerces, pour optimiser leur disponibilité et leur visibilité.
\item Étudier les attributs les plus fréquents des commerces, pour identifier les services les plus recherchés par les utilisateurs.
\item Analyser les tendances au fil du temps, en examinant les checkins et les tips sur une période donnée.
\end{itemize}

Nous pensons que ces analyses permettront de mieux comprendre les tendances et les préférences des utilisateurs, ainsi que les caractéristiques des commerces les plus populaires. Ces informations peuvent être utiles pour les propriétaires d’établissements, les gestionnaires de franchises ou les analystes de marché qui souhaitent optimiser leurs stratégies commerciales et leur présence en ligne.
Par exemple, en identifiant les villes les plus attractives,
les commerçants peuvent cibler leurs efforts de marketing et d’expansion pour maximiser leur visibilité et leur rentabilité.
De même, en analysant les horaires d’ouverture les plus courants, ils peuvent ajuster leurs plages horaires pour répondre aux besoins de leur clientèle et augmenter leur fréquentation.

\chapter*{Conception du Data Warehouse}


\section*{4.1 Approche méthodologique}

Pour la conception de notre data warehouse, nous avons choisi d’adopter l'approche de Kimball, qui privilégie une modélisation orientée métier et centrée sur les besoins des utilisateurs finaux.
Cette approche se distingue par sa simplicité, sa flexibilité et sa capacité à s’adapter aux évolutions des données et des besoins analytiques.
Elle repose sur des concepts clés tels que les faits, les dimensions, les grains de granularité et les schémas en étoile ou en flocon.

Nous avons opté pour un schéma en étoile pour notre data warehouse, en raison de sa simplicité et de sa facilité d’utilisation pour les analyses multidimensionnelles.
Ce schéma est composé d’une table de faits centrale, entourée de tables de dimensions qui décrivent les contextes et les attributs des données.
Cette structure en étoile permet de simplifier les requêtes et d’optimiser les performances des analyses.
L'approche de Inmon, qui privilégie une modélisation normalisée et une intégration des données en amont, aurait pu être une alternative, mais nous avons préféré la flexibilité et la simplicité offertes par le schéma en étoile.
Notre conception prend en compte une evolution future des besoins et des données, en permettant l’ajout de nouvelles dimensions ou de nouveaux faits sans impacter l’existant.

\section*{4.2 Spécifications des schémas}

Avec les questions et les besoins identifiés, nous avons défini les schémas suivants pour notre data warehouse :

\begin{itemize}
\item Table de faits "fact_business" : Contient les mesures et les indicateurs clés liés aux commerces, tels que la géographie, le nombre de checkins, les attributs, les catégories et le nombre de reviews.

\item Tables de dimensions :
\begin{itemize}
\item "dim_business" : Décrit les caractéristiques des commerces.
\item "dim_hours" : Décrit tous les horaires possibles d'ouverture des commerces.
\item dim_business_hours : Décrit les horaires d'ouverture des commerces.
\item dim_amenagement : Décrire les aménagements des commerces.
\item dim_tips : Décrit les avis donnés par les utilisateurs sur les commerces.
\item dim_city : Décrit les villes en fonction de leur nombre d'habitants et de leur pays.
\item dim_checkin : Décrit les dates des checkins effectués par les utilisateurs dans les commerces.
\end{itemize}

\end{itemize}

Nous pensons que ces schémas permettront de répondre efficacement aux questions posées et de fournir des analyses pertinentes et exploitables pour les utilisateurs finaux.

Les requêtes destiné à la création de ces tables sont disponibles dans le dossier "/sql/load/creating_dim_tables_2.sql".


\section*{4.3 Justifications des choix techniques}

Pour la mise en œuvre de notre data warehouse, nous avons choisi les technologies suivantes :

\begin{itemize}

\item Python : Nous avons choisi d’utiliser le langage Python avec différentes librairies comme Pandas pour implémenter notre ETL. Nous avons opté pour cette solution en raison de sa simplicité et l'aisance que nous avons grâce aux séances de l'UE Machine Learning et Deep Learning, où nous avons utilisé Pandas pour manipuler des données dans des fichiers CSV et JSON\@.
\item SGBD (temporaire) SQLite : Le choix de SQLite s’est imposé pour plusieurs raisons. Tout d’abord, il s’agit d’une solution légère et intégrée qui ne nécessite aucune configuration complexe ni serveur dédié,
ce qui simplifie le déploiement et la maintenance. Ensuite, SQLite est entièrement autonome et stocke les données dans un seul fichier, ce qui facilite grandement le transport et le
partage de la base de données. Enfin, sa compatibilité avec la plupart des systèmes d’exploitation et son support natif dans Python en font une option pratique et efficace pour
gérer les données du projet. Nous voulons que les données soient stockées dans un format tabulaire pour faciliter leur manipulation et leur analyse ultérieure.
SQLite offre une solution simple et efficace pour stocker des données structurées dans des tables relationnelles, ce qui en fait un choix idéal pour cette tâche.
\item SGBD (production) PostgreSQL : Pour stocker les données et exécuter les requêtes analytiques.
PostgreSQL est un SGBD open-source (contrairement à Oracle) qui offre des fonctionnalités avancées pour les entrepôts de données, telles que les index avancés, les vues matérialisées et les procédures stockées.
Il est également compatible avec de nombreux outils de business intelligence, ce qui facilite l’analyse et la visualisation des données. Pour voir plus loin, avec l'extension PostGIS, nous pourrions effectuer des analyses géospatiales avancées avec les positions géographiques des commerces et des utilisateurs.
\item SQL : Le choix d'opter pour SQL est motivé par sa flexibilité notable car ce langage est compatible avec de nombreux SGBD. Cela permet de migrer facilement vers un autre SGBD si nécessaire, sans modifier considérablement les scripts existants.
Cette portabilité, associée à la puissance de SQL pour manipuler et structurer efficacement les données en fait un outil particulièrement adapté à ce projet nottament sur le fait que nous n'avons pas de calculs complexes à effectuer.
\item Metabase : Pour créer des tableaux de bord interactifs et des visualisations des données. Metabase est un outil open-source qui offre une interface intuitive pour explorer les données et générer des rapports visuels.
Il est compatible avec PostgreSQL et ou SQLite et permet de créer des graphiques, des cartes et des tableaux de bord personnalisés pour communiquer efficacement les résultats des analyses.
\end{itemize}

Ces choix nous permettent egalement de pouvoir travailler en dehors de la fac sans recontrer de différence dans les outils utilisés, ce qui est un avantage non négligeable pour un projet de groupe.

\chapter*{Intégration des données}


\section*{5.1 Architecture de l’ETL}

Un ETL est un processus d’extraction, de transformation et de chargement des données dans un entrepôt de données. Il est essentiel pour garantir la qualité et la cohérence des données.

Voici le schéma général de notre processus ETL, avec les différents scripts et étapes impliquées :


Chacune des étapes de l’ETL sera détaillée dans les sections suivantes, en mettant en évidence l'approche adoptée, les problèmes rencontrés et les solutions apportées.

\subsection{5.1.1 Extraction des données}

Via des scripts Python développés pour chaque fichier, nous avons extrait les données et les avons stockées dans une base de données SQLite.
Ces scripts sont disponibles dans le dossier "/script" du projet.

Nous voulons que les données soient stockées dans un format tabulaire pour faciliter leur manipulation et leur analyse ultérieure.

Voici un schéma de l'extraction des données et le schéma relationnel qui en découle :


Chacune des étapes de l’ETL sera détaillée dans les sections suivantes, en mettant en évidence l'approche adoptée, les problèmes rencontrés et les solutions apportées.

\subsection{5.1.2 Transformation des données}

La transformation des données est une étape cruciale de l’ETL, qui consiste à nettoyer, normaliser et enrichir les données brutes pour les rendre cohérentes et exploitables.
Cette etape implique d'avoir une connaissance globale des données et de leur contexte pour pouvoir les traiter correctement.
Par exemple, il y a des patterns que l'on peut retrouver dans les données, comme les dates, la taille de certains champs (business_id ou user_id), des champs exclusivement numériques, etc.
Ces patterns nous permettent de détecter des valeurs érronées et de les traiter en conséquence.
Pour cela, nous avons développé des opérations de type DELETE pour nettoyer les données et les rendre cohérentes.
Le nettoyage d'un fichier est associé à un fichier SQL qui contient les requêtes de nettoyage à effectuer sur la table associée.
Cette décision a été prise pour faciliter la maintenance et la réutilisation des scripts de nettoyage.

Voici un schéma qui résume la portée des actions des différents fichier SQL de nettoyage :


\subsection{5.1.3 Chargement des données}

La dernière étape de l’ETL consiste à charger les données transformées dans le data warehouse. Ces données sont stockées dans des tables spécifiques, prêtes à être
analysées par des outils de business intelligence.

Nous avons créé un script Python qui se connecte à la base de données PostgreSQL et charge les données à partir de la base SQLite.
Comme les données que nous devons charger sont plutôt "volumineuses" (~1.5Go), nous avons choisi de d'abord exporter chaque table de la base SQLite en fichier CSV, puis de les
importer dans la base PostgreSQL via la commande COPY. L'avantage de cette commande est qu'elle est très rapide et efficace pour charger de grandes quantités de données.
Contrairement à une insertion ligne par ligne, COPY permet de charger les données en blocs, ce qui réduit considérablement le temps de chargement.

Voici un schéma qui résume le processus de chargement des données :

Ce script se trouve dans le répertoire   "script/load/".

\section{5.3 Problèmes rencontrés et solutions apportées}

Dans cette section, nous décrivons les problèmes que nous avons rencontrés lors de ces processus et les solutions que nous avons mises en place pour les résoudre.

\subsection{5.3.1 Problèmes rencontrés lors de l'extraction des données}


\subsubsection{5.3.1.1 Fichier JSON}

Le problème principal que nous avons rencontré lors de l'extraction des données est la gestion des fichiers JSON.
En effet, ces fichiers sont structurés de manière complexe et imbriquée, ce qui rend leur traitement plus difficile que les fichiers CSV.
Pour résoudre ce problème, nous avons dû développer des scripts Python spécifiques pour extraire les données pertinentes et les stocker dans des tables avant de les intégrer dans la base de données finale.
Lors de cette phase, nous n'avons pas "perdu" de données, mais nous avons dû faire des choix sur les informations à extraire et à stocker en fonction de leur pertinence pour les analyses futures.

\subsubsection{5.3.1.2 Fichier CSV}

Le problème rencontré avec le fichier CSV etait sa non conformité avec le schéma de la base de données.
En effet, ce fichier qui contient 4 colonnes, contient des lignes qui ont plus de 4 colonnes.
Cela est dû à la présence de virgules dans les champs textuels qui n'ont pas été correctement échappées et donc qui ont été interprétées comme des séparateurs de colonnes.
On se retrouve donc avec des lignes que l'on doit volontairement ignorer car elles ne sont pas exploitables.
Lors de cette partie, nous avons perdu 5.23% des données.
Le nombre total de lignes du fichier CSV est de 1375578 et le nombre de lignes ignorées est de 58243 soit 1317335 lignes qui ont été insérées dans la base de données.
Ce nombre de lignes est suffisant pour effectuer des analyses pertinentes et significatives, par contrainte de temps, nous avons choisi de ne pas traiter ces lignes érronées.


\subsubsection{5.3.1.3 API}

La principale difficulté à tout d'abord été de trouver une API qui répondait à nos besoins.
Nous avons trouvé une API fiable qui nous permettait de récupérer des données démographiques en fonction du nom de la ville.
Sa particularité est qu'elle est gratuite et qu'elle ne nécessite pas de clé d'API, nous sommes donc limités en nombre de requêtes par jour (1000/heure).

Voici un exemple de cette requête :

\begin{verbatim}
http://api.geonames.org/search?q={city}&maxRows=1&type=json&username={username}
\end{verbatim}

Un exemple de réponse est le suivant :

\begin{verbatim}
{
"totalResultsCount": 1701,
"geonames": [
{
"adminCode1": "08",
"lng": "-79.39864",
"geonameId": 6167865,
"toponymName": "Toronto",
"countryId": "6251999",
"fcl": "P",
"population": 2600000,
"countryCode": "CA",
"name": "Toronto",
"fclName": "city, village,...",
"adminCodes1": {
"ISO3166_2": "ON"
},
"countryName": "Canada",
"fcodeName": "seat of a first-order administrative division",
"adminName1": "Ontario",
"lat": "43.70643",
"fcode": "PPLA"
}
]
}
\end{verbatim}

Pour notre usage, nous n'utilisons que les champs "population" et "countryCode. Nous avons jugé que les autres champs n'étaient pas pertinents pour nos analyses.

Nous avons pu quantifier le nombre de villes à traiter en faisant une requete d'agrégation sur le nom de la ville.
Nous avons trouvé ~ 1200 villes différentes, ce qui est un nombre raisonnable en terme de requêtes à effectuer et de données à stocker.

\subsection{5.3.2 Problèmes rencontrés lors de la transformation des données}

Lors de la transformation des données, notre problème principal à été de nettoyer et de normaliser les données brutes pour les rendre cohérentes et exploitables pour l’analyse.
Cela implique de comprendre nos données et leurs caractéristiques.
Pour cela, nous avons exploré nos données en utilisant des requêtes SQL sur notre base de données.

\subsubsection{5.3.2.1 Fichier CSV}

Pour le fichier "yelp_academic_dataset_tip.csv", nous avons détecté des valeurs érronées dans toutes les colonnes.

Voici les elements qui nous ont permis de détecter ces valeurs érronées :

\begin{itemize}
\item Colonnes "user\_id" et "business\_id" : Un user\_id ou un business\_id ne peuvent pas être vide, ils ont une taille de 22 caractères et ne contiennent aucun espace.
Pour filtrer les lignes incorrectes, nous avons utilisé la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(business_id) != 22
OR business_id GLOB '* *'
OR length(user_id) != 22
OR user_id GLOB '* *'
OR user_id IS NULL
OR business_id IS NULL;
\end{verbatim}

On supprime donc les lignes qui ne respectent pas ces conditions et on est sûr que les données restantes sont cohérentes.

\item Colonne "compliment_count" : Cette colonne doit contenir des entiers positifs entre 0 et 10. On supprime donc les lignes qui ne respectent pas ces conditions via la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE compliment_count GLOB '*[^0-9]*'
OR compliment_count IS NULL;
\end{verbatim}

\item Colonne "date" : Cette colonne doit contenir des dates au format DATETIME. On supprime donc les lignes qui ne respectent pas ces conditions avec la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(date) != 19
OR date NOT GLOB '????-??-?? ??:??:??'
OR date IS NULL;
\end{verbatim}


Voici le tableau qui résume les données avant et après le nettoyage :

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Etape \& Nombre de lignes \\& Pourcentage de lignes conservées \\
\hline
Fichier brut \& 1375578 \& 100\% \\
\hline
Insertion \& 1317335 \& 95,76592531\% \\
\hline
Nettoyage \& 1315972 \& 95,66683968\% \\
\hline
\end{tabular}
\end{center}

Nous trouvons que le pourcentage de lignes conservées entre le fichier brut et le fichier nettoyé reste suffisamment élevé pour effectuer des analyses pertinentes et significatives.


\subsubsection{5.3.2.2 Fichier JSON}

Pour les fichiers "yelp_academic_dataset_business.json" et "yelp_academic_dataset_checkin.json", nous n'avons pas trouvé de problèmes particuliers, les données étaient déjà bien structurées et cohérentes.
Nous avons pu détecter cela en faisant des comparatifs avant et après le nettoyage des données.

Voici un tableau qui résume les données avant et après le nettoyage :

Nous arrivons donc à extraire et à nettoyer les données de ces fichiers sans aucune perte de données. Toutefois, des champs dans certaines tables sont vides, par exemple, pour un commerce,
l'adresse peut être vide, nous avons juger que supprimer des lignes pour ces cas n'était pas pertinent.

Nous avons également remarqué des problèmes de doublon dans le nom des villes comme illustrés via la figure ci dessous :

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{images/doublon_ville.png}
\caption{Doublon dans le nom des villes}
\end{figure}

Via cet exemple, nous avons détecté un problème de qualité des données qui impactera les analyses futures.
Par exemple, en voulant regrouper les données avec un GROUP BY sur le nom de la ville, nous aurons plusieurs résultats qui correspondent à la même ville, ce qui faussera les résultats.

Pour résoudre ce problème, plusieurs solutions sont envisageables comme par exemple:

\begin{itemize}
\item Utiliser une fonction de nettoyage des données pour supprimer les espaces en trop, les caractères spéciaux, etc.
\item Utiliser une fonction de normalisation des données pour uniformiser les noms des villes.
\item Utiliser une fonction de déduplication pour fusionner les noms de villes similaires.
\item Utiliser un algorithme de distance de Levenshtein pour regrouper les noms de villes proches.
\end{itemize}

Nous avons choisi d'utiliser une fonction de nettoyage des données pour supprimer les espaces en trop et les caractères spéciaux car c'est la solution la plus simple et la plus rapide à mettre en place et de croisé le nom des villes à la table de dimension des villes.
En faisant une jointure sur la table des villes en ignorant la casse, nous avons pu regrouper les noms des villes similaires et les normaliser, corrigeant ainsi le problème de doublon.

\subsubsection{5.3.2.1 API}

Pour l'API, nous n'avons pas rencontré de problèmes majeurs, les données retournées étaient cohérentes et bien structurées.
Il y a toutefois des villes qui n'ont pas été trouvées dans la base de données de l'API.
Nous avons decidé de les laisser en l'état car elles ne représentent qu'une petite partie des données et permettent de corriger le problème de doublon expliqué précédemment.


\subsection{5.3.3 Problèmes rencontrés lors du chargement des données}

Lors du chargement des données, nous n'avons pas rencontré de problèmes majeurs.
La commande COPY de PostgreSQL s'est avérée très efficace pour charger les données en blocs et réduire considérablement le temps de chargement.
Ce processus à une durée en moyenne de moins de 30 secondes pour l'ensemble des opération de chargement des données.
Par manque de temps, nous n'avons pas fais de comparatif entre la méthode COPY et une insertion ligne par ligne, mais nous sommes convaincus que la méthode COPY est la plus efficace pour charger de grandes quantités de données.
Après le chargement des données, nous n'avons pas detecté d'anomalies sur nos données dans le SGBD PostgreSQL (kafka.iem),

Voici un comparatif du volume occupé en Mo par chaque table dans la base SQLite et dans la base PostgreSQL :

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Table & SQLite (Mo) & PostgreSQL (Mo) \\
\hline
tips & 50 & 50 \\
\hline
\end{tabular}
\end{center}

L'intéret ici, est de voir la taille des données entre deux SGDB différents


\chapter*{Présentation des résultats}


\item 6.1 Tableaux de bord et visualisations

La partie de visualisation des données est une étape cruciale pour communiquer efficacement les résultats des analyses et les insights obtenus.
Notre but a été d'exprimer les résultats des différentes requêtes dans la forme la plus claire et la plus compréhensible possible pour les utilisateurs finaux.
Metabase fournit de bons outils pour répondre à ce besoin.

Les tableaux de bord et les visualisations ont été regroupé en catégories comme comme enoncé dans la section sur l'analyse des besoins, cela permet de faciliter la navigation et de rendre les informations plus accessibles.

Voici un aperçu des tableaux de bord et des visualisations créés :



La majorité de ces graphiques utilisent des reqêtres que nous avons préalablement écrites dans un query editor.
Ces requêtes sont disponibles dans le dossier "sql/bi/report/metabase.sql".

\item 6.2 Synthèse des résultats analytiques

Nous jugeons que les résultats obtenus sont pertinents et exploitables pour les utilisateurs finaux.
Les analyses réalisées permettent de mieux comprendre les tendances et les caractéristiques des commerces, ainsi que les préférences des utilisateurs.
Les visualisations offrent une vue synthétique et intuitive des données.

Voici une analyse que nous avons pu tirer des données :

\begin{itemize}

\item On constate une baisse de l'utilisation de la plateforme Yelp au fil du temps, avec une diminution du nombre de checkins et de tips. (Analyse sur les tendances)

\item Les restaurants sont les commerces les plus nombreux sur la plateforme, suivis par les magasins et les services.

\item Les villes les plus populaires sont Las Vegas, Phoenix et Toronto, avec un nombre élevé de commerces et d'utilisateurs.

\item L'attribut le plus commun est d'avoir le "Wi-Fi"et la catégorie la plus commune est "Restaurants".

\item Les heures d'ouverture les plus courantes sont de 8h à 17h, du lundi au vendredi.


\item 6.3 Analyse des performances

Pour évaluer les performances de notre data warehouse, nous avons mesuré le temps d’exécution de chargement des différents rapport analytiques.
Metabase fournit des outils de monitoring et de suivi des performances qui nous ont permis de mesurer les temps de réponse des requêtes et d’identifier les éventuels goulets d’étranglement.

Voici un tableau récapitulatif des performances des requêtes :

\begin{center}
\begin{tabular}{|c|c|}
\hline
Requête & Temps d'exécution \\
\hline
Requête 1 & 0.5s \\
\hline
Requête 2 & 0.7s \\
\hline
Requête 3 & 1.2s \\
\hline
\end{tabular}
\end{center}

Nous avons jugé que ces temps d'exécution étaient satisfaisants pour les analyses réalisées et les volumes de données traités.

Le SGBD que nous utilisons possède une bonne configuration matérielle et logicielle, en plus de bénéficier de la proximité de ce dernier, en découlent des temps d'exécution rapides.



\chapter*{Documentation technique}



\item 8.1 Liste des dépendances

Pour l’exécution de l'ensemble du processus ETL, voici les étapes à suivre pour installer les dépendances nécessaires :

\begin{itemize}
\item Installer Python 3.12 et executrer la commande suivante pour installer les librairies nécessaires :

\begin{verbatim}
pip install -r requirements.txt
\end{verbatim}

\item Installer PostgreSQL et configurer les paramètres de connexion dans le fichier "/script/load/"

\item Installer Metabase et configurer les paramètres de connexion à la base de données PostgreSQL.
\end{itemize}


\item 8.2 Instructions pour l’exécution du code

Une fois les dépendances installées, il suffit d'executer le fichier run.sh qui se chargera d'effectuer toutes les étapes de l'ETL.


\chapter*{Bilan}

\subsection{8.1 Synthèse}

Nous pensons avoir respecté les objectifs initiaux du projet en construisant un data warehouse fonctionnel et en réalisant des analyses pertinentes sur les données Yelp.

Voici un schéma qui résume le temps passé sur chaque étape du projet :


La partie sur la conception du data warehouse a été la plus longue car nous devions definir au préalable ce que nous voulions analyser et quelles données étaient nécessaires pour répondre à ces questions.
La création des rapports avec Metabase à neccessité de prendre de la hauteur pour qu'un public non technique puisse comprendre les résultats de nos analyses.


\subsection{8.2 Perspectives}

Pour aller plus loin, voici quelques pistes d’amélioration et d’extension du projet :

\begin{itemize}

\item Intégration de nouvelles sources de données : Pour enrichir les analyses et les visualisations, il serait intéressant d’intégrer des données supplémentaires, telles que des données météorologiques, des données économiques ou des données sociales.
Cela permettrait de mieux comprendre les facteurs qui influencent les commerces et les utilisateurs, en croisant différentes sources d’information.

\item Implémentation de modèles prédictifs : Pour anticiper les tendances et formuler des prévisions, il serait utile de développer des modèles prédictifs basés sur les données existantes.
Par exemple, en utilisant des techniques de machine learning, il serait possible de prédire les notes des commerces ou les comportements des utilisateurs en fonction de différents facteurs.

\item Développement de fonctionnalités avancées : Pour améliorer l’expérience utilisateur et offrir des fonctionnalités avancées, il serait intéressant d’ajouter des outils de recommandation, des analyses prédictives ou des fonctionnalités de géolocalisation.
Cela permettrait de personnaliser les résultats en fonction des préférences des utilisateurs et de proposer des recommandations pertinentes.

\item Outils de BI plus avancés : Pour améliorer les visualisations et les tableaux de bord, il serait utile d’explorer des outils de business intelligence plus avancés, tels que Power BI ou QlikView.

\end{itemize}

\subsection{8.3 Conclusion}

En conclusion, ce projet nous a permis de mettre en pratique les concepts fondamentaux de l’informatique décisionnelle et de la modélisation de données.
Nous avons exploré les différentes étapes du cycle de vie d’un data warehouse, de la modélisation à l’analyse en passant par l’intégration des données.
Nos compétences en SQL ont été renforcées et nous avons acquis une expérience précieuse dans la manipulation de données massives et hétérogènes.
Les outils de BI ont permis de souligner l’importance de la visualisation des données pour communiquer efficacement les résultats des analyses.


\chapter*{Annexes}


