\section*{Résumé}


\section*{Introduction}

\subsection{Contexte}\label{subsec:contexte}
Dans le cadre du module d’Informatique Décisionnelle du Master 2 BDIA, ce projet propose d’explorer et d’appliquer les concepts fondamentaux de la création et de l’exploitation d’un data warehouse, un élément clé dans les environnements décisionnels modernes. L’importance croissante des données massives a profondément transformé les processus de prise de décision des entreprises, les poussant à s’appuyer sur des analyses précises et des outils avancés pour rester compétitifs.

Dans ce contexte, ce projet simule des problématiques réelles rencontrées par des analystes, des décideurs ou des responsables d’entreprise souhaitant exploiter des données volumineuses pour identifier des tendances, formuler des prévisions ou optimiser leurs opérations. Les données proviennent de l’application Yelp, une plateforme reconnue qui regroupe des millions d’avis d’utilisateurs sur divers commerces. 

Ce projet couvre l’ensemble des étapes essentielles du cycle de vie d’un système décisionnel : modélisation, intégration, analyse et présentation des données. 


\section*{2.2 Objectifs}
L’objectif principal de ce projet est de construire un système décisionnel complet et fonctionnel à partir d’un jeu de données volumineux et hétérogène fourni par l’application Yelp. Cela implique :

\begin{enumerate}
\item La modélisation d’un data warehouse adapté aux besoins analytiques identifiés, en choisissant entre des approches méthodologiques reconnues (Kimball ou Inmon).
\item La conception et la mise en œuvre d’un processus ETL pour intégrer des données provenant de multiples formats et sources, tout en assurant leur cohérence et leur disponibilité pour l’analyse.
\item L’élaboration de requêtes analytiques pertinentes, permettant d'explorer les données et de répondre à des questions stratégiques, telles que l’identification des commerces les mieux évalués ou la segmentation des utilisateurs selon leurs préférences.
\item La création de tableaux de bord interactifs et intuitifs, permettant de synthétiser les résultats et de les communiquer efficacement à un public non technique.
\end{enumerate}

Pour la conception de notre data warehouse, nommé “Argon” en référence à l’élément chimique réputé pour sa stabilité et sa résistance, nous avons visé une architecture robuste et performante. Ce nom symbolise un système capable de centraliser et protéger les données tout en offrant une grande efficacité pour les analyses. Inspirée des propriétés de l’argon, notre conception met l’accent sur la simplicité, l’optimisation des performances, et l’isolation des composants pour garantir flexibilité et résilience face aux évolutions des données. 


Nos analyses se concentrent sur les commerces, en mettant l'accent sur les facteurs clés qui influencent leur implantation et leur succès dans différents lieux, en exploitant les données géographiques, les avis utilisateurs et les caractéristiques spécifiques des établissements.


\chapter*{Analyse du sujet et des données}


\section*{3.1 Description des données disponibles}
Les données exploitées dans ce projet proviennent de la plateforme Yelp et sont organisées sous différents formats et sources. Elles offrent une vue détaillée des commerces, des utilisateurs et de leurs interactions. Voici une présentation des principales sources de données :


\subsection*{3.1.1. Fichiers CSV et JSON}

Fichier CSV "tip" : Contient des avis courts et succincts donnés par les utilisateurs. Ces tips incluent des informations comme le contenu textuel, l’identifiant de l’utilisateur, la date et le commerce concerné.


Fichiers JSON :

\begin{itemize}
\item "checkin" : Liste des visites (checkins) effectuées dans les commerces. Ces données incluent des horodatages précis, permettant de suivre la fréquentation des établissements sur une période donnée.


\item "business" : Informations détaillées sur les commerces, telles que leur localisation, leurs catégories, leurs horaires d'ouverture et des attributs supplémentaires (présence d’un parking, options alimentaires, etc.).


\end{itemize}

\subsection*{3.1.2. Tables dans la base PostgreSQL}
La base de données PostgreSQL contient plusieurs tables liées aux utilisateurs et aux avis :

\begin{itemize}
\item Table "yelp.user" : Fournit des informations sur les utilisateurs, telles que leur identifiant unique et les statistiques associées à leur activité (nombre de contributions, réactions reçues, etc.).


\item Table "yelp.friend" : Décrit les relations d’amitié entre utilisateurs, permettant de modéliser les réseaux sociaux et leurs impacts potentiels sur les évaluations.


\item Table "yelp.elite" : Indique les années où les utilisateurs ont obtenu le statut d’élite.


\item Table "yelp.review" : Contient les évaluations des utilisateurs avec des notes, des commentaires textuels et des métadonnées supplémentaires, telles que la date et l’identifiant du commerce évalué.


\end{itemize}

Voici le diagramme relationnel de ce schéma de base de données :

\section*{3.2 Analyse des besoins}

Pour répondre aux besoins des utilisateurs finaux, nous avons identifié plusieurs cas d’usage et scénarios d’analyse qui nécessitent des requêtes complexes et des agrégations de données.
Voici quelques exemples de questions auxquelles nous souhaitons répondre organisé par thème :

\begin{itemize}
\item Analyse de la géographie :
\begin{itemize}
\item Quels sont les 20 villes avec le plus de commerces ?
\item Quels sont les villes qui donnent en moyenne le plus de compliments ?
\item Quels sont les Etats qui font en moyenne le plus de checkins ?
\end{itemize}
\item Analyse des commerces :
\begin{itemize}
\item Quels sont les 200 business avec le plus de compliments
\end{itemize}
\item Analyse des horaires :
\begin{itemize}
\item Quels sont les heures d'ouverture les plus communes sur une semaine ?
\item Quels sont les heures d'ouverture les plus communes par jour de la semaine ?
\item Combien de business sont ouverts 24h/24 7j/7 ?
\end{itemize}
\item Analyse des attributs :
\begin{itemize}
\item Quels sont les attributs les plus courants des commerces ?
\item Quels sont les catégories les plus communes par ville ?
\item Quels sont les catégories les plus communes à Las Vegas ?
\item Quels sont les catégories qui plaisent le plus ?
\end{itemize}
\end{itemize}

Ces questions nous permettront de mieux comprendre les tendances et les caractéristiques des commerces, ainsi que les préférences des utilisateurs.
Elles guideront la conception de notre data warehouse et l’élaboration des requêtes analytiques.


\section*{3.3 Définition des objectifs analytiques}

Sur la base des besoins identifiés, nous avons défini les objectifs analytiques suivants :

\begin{itemize}
\item Identifier les villes les plus attractives pour les commerces, en fonction de leur popularité.
\item Comprendre quels sont les commerces les mieux évalués, en analysant les avis et les notes des utilisateurs.
\item Analyser les horaires d’ouverture les plus courants des commerces, pour optimiser leur disponibilité et leur visibilité.
\item Étudier les attributs les plus fréquents des commerces, pour identifier les services les plus recherchés par les utilisateurs.
\end{itemize}

Nous pensons que ces analyses permettront de mieux comprendre les tendances et les préférences des utilisateurs, ainsi que les caractéristiques des commerces les plus populaires. Ces informations peuvent être utiles pour les propriétaires d’établissements, les gestionnaires de franchises ou les analystes de marché qui souhaitent optimiser leurs stratégies commerciales et leur présence en ligne.
Par exemple, en identifiant les villes les plus attractives, les commerçants peuvent cibler leurs efforts de marketing et d’expansion pour maximiser leur visibilité et leur rentabilité. De même, en analysant les horaires d’ouverture les plus courants, ils peuvent ajuster leurs plages horaires pour répondre aux besoins de leur clientèle et augmenter leur fréquentation.

\chapter*{Conception du Data Warehouse}


\section*{4.1 Approche méthodologique}

Pour la conception de notre data warehouse, nous avons choisi d’adopter l'approche de Kimball, qui privilégie une modélisation orientée métier et centrée sur les besoins des utilisateurs finaux.
Cette approche se distingue par sa simplicité, sa flexibilité et sa capacité à s’adapter aux évolutions des données et des besoins analytiques.
Elle repose sur des concepts clés tels que les faits, les dimensions, les grains de granularité et les schémas en étoile ou en flocon.

Nous avons opté pour un schéma en étoile pour notre data warehouse, en raison de sa simplicité et de sa facilité d’utilisation pour les analyses multidimensionnelles.
Ce schéma est composé d’une table de faits centrale, entourée de tables de dimensions qui décrivent les contextes et les attributs des données.
Cette structure en étoile permet de simplifier les requêtes et d’optimiser les performances des analyses.
L'approche de Inmon, qui privilégie une modélisation normalisée et une intégration des données en amont, aurait pu être une alternative, mais nous avons préféré la flexibilité et la simplicité offertes par le schéma en étoile.
Notre conception prend en compte une evolution future des besoins et des données, en permettant l’ajout de nouvelles dimensions ou de nouveaux faits sans impacter l’existant.

\section*{4.2 Spécifications des schémas}

Avec les questions et les besoins identifiés, nous avons défini les schémas suivants pour notre data warehouse :

\begin{itemize}
\item Table de faits "fact\_business" : Contient les mesures et les indicateurs clés liés aux commerces, tels que les notes moyennes, les nombres de compliments, etc.
\item Tables de dimensions :
\begin{itemize}
\item "dim\_business" : Décrit les caractéristiques des commerces, telles que leur localisation, leurs horaires d’ouverture, leurs catégories, etc.
\item "dim\_category" : Regroupe les catégories de commerces, pour permettre des analyses par secteur d’activité.
\item "dim\_city" : Fournit des informations sur les villes, telles que leur population, leur superficie, etc.



\section*{4.3 Justifications des choix techniques}

Pour la mise en œuvre de notre data warehouse, nous avons choisi les technologies suivantes :

\begin{itemize}

\item Python : Nous avons choisi d’utiliser le langage Python avec différentes librairies comme Pandas pour implémenter notre ETL. Nous avons opté pour cette solution en raison de sa simplicité et l'aisance que nous avons grâce aux séances de l'UE Machine Learning et Deep Learning, où nous avons utilisé Pandas pour manipuler des données dans des fichiers CSV et JSON\@.
\item SGBD (temporaire) SQLite : Le choix de SQLite s’est imposé pour plusieurs raisons. Tout d’abord, il s’agit d’une solution légère et intégrée qui ne nécessite aucune configuration complexe ni serveur dédié,
ce qui simplifie le déploiement et la maintenance. Ensuite, SQLite est entièrement autonome et stocke les données dans un seul fichier, ce qui facilite grandement le transport et le
partage de la base de données. Enfin, sa compatibilité avec la plupart des systèmes d’exploitation et son support natif dans Python en font une option pratique et efficace pour
gérer les données du projet. Nous voulons que les données soient stockées dans un format tabulaire pour faciliter leur manipulation et leur analyse ultérieure.
SQLite offre une solution simple et efficace pour stocker des données structurées dans des tables relationnelles, ce qui en fait un choix idéal pour cette tâche.
\item SGBD (production) PostgreSQL : Pour stocker les données et exécuter les requêtes analytiques.
PostgreSQL est un SGBD open-source (contrairement à Oracle) qui offre des fonctionnalités avancées pour les entrepôts de données, telles que les index avancés, les vues matérialisées et les procédures stockées.
Il est également compatible avec de nombreux outils de business intelligence, ce qui facilite l’analyse et la visualisation des données. Pour voir plus loin, avec l'extension PostGIS, nous pourrions effectuer des analyses géospatiales avancées avec les positions géographiques des commerces et des utilisateurs.
\item Metabase : Pour créer des tableaux de bord interactifs et des visualisations des données. Metabase est un outil open-source qui offre une interface intuitive pour explorer les données et générer des rapports visuels.
Il est compatible avec PostgreSQL et ou SQLite et permet de créer des graphiques, des cartes et des tableaux de bord personnalisés pour communiquer efficacement les résultats des analyses.
\end{itemize}


\chapter*{Intégration des données}


\section*{5.1 Architecture de l’ETL}

Un ETL est un processus d’extraction, de transformation et de chargement des données dans un entrepôt de données. Il est essentiel pour garantir la qualité et la cohérence des données.

Voici le schéma général de notre processus ETL, avec les différents scripts et étapes impliquées :


\subsection{5.1.1 Extraction des données}

Via des scripts Python développés pour chaque fichier, nous avons extrait les données et les avons stockées dans une base de données SQLite.
Ces scripts sont disponibles dans le dossier "/script" du projet.

Le choix de SQLite s’est imposé pour plusieurs raisons. Tout d’abord, il s’agit d’une solution légère et intégrée qui ne nécessite aucune configuration complexe ni serveur dédié,
ce qui simplifie le déploiement et la maintenance. Ensuite, SQLite est entièrement autonome et stocke les données dans un seul fichier, ce qui facilite grandement le transport et le
partage de la base de données. Enfin, sa compatibilité avec la plupart des systèmes d’exploitation et son support natif dans Python en font une option pratique et efficace pour
gérer les données du projet.

Ici, SQLite est utilisé comme un entrepôt de données temporaire pour stocker les données brutes avant de les transformer et de les charger dans le data warehouse final.
Nous voulons que les données soient stockées dans un format tabulaire pour faciliter leur manipulation et leur analyse ultérieure.

Voici un schéma de l'extraction des données et le schéma relationnel qui en découle :

\subsection{5.1.2 Transformation des données}

La transformation des données est une étape cruciale pour nettoyer, normaliser et enrichir les données brutes, afin de les rendre cohérentes et exploitables pour l’analyse.
Elle inclut des opérations clés telles que le filtrage, le tri, la jointure, l’agrégation et la déduplication, essentielles pour garantir la qualité et la pertinence des données.
Nous avons implémenté ces transformations via des fichiers SQL contenant les requêtes nécessaires.
L’utilisation de SQL offre une flexibilité notable car ce langage est compatible avec de nombreux SGBD.
Cela permet de migrer facilement vers un autre SGBD si nécessaire, sans modifier considérablement les scripts existants.
Cette portabilité, associée à la puissance de SQL pour manipuler et structurer efficacement les données en fait un outil particulièrement adapté à ce projet nottament sur le fait que nous n'avons pas de calculs complexes à effectuer.


\subsection{5.1.3 Chargement des données}

La dernière étape de l’ETL consiste à charger les données transformées dans le data warehouse. Ces données sont stockées dans des tables spécifiques, prêtes à être
analysées par des outils de business intelligence.

Nous avons créé un script Python qui se connecte à la base de données PostgreSQL et charge les données à partir de la base SQLite.
Comme les données que nous devons charger sont plutôt "volumineuses" (~1.5Go), nous avons choisi de d'abord exporter chaque table de la base SQLite en fichier CSV, puis de les
importer dans la base PostgreSQL via la commande COPY. L'avantage de cette commande est qu'elle est très rapide et efficace pour charger de grandes quantités de données.
Contrairement à une insertion ligne par ligne, COPY permet de charger les données en blocs, ce qui réduit considérablement le temps de chargement.

Voici un schéma qui résume le processus de chargement des données :


\section{5.3 Problèmes rencontrés et solutions apportées}

Dans cette section, nous décrivons les problèmes que nous avons rencontrés lors de ces processus et les solutions que nous avons mises en place pour les résoudre.

\subsection{5.3.1 Problèmes rencontrés lors de l'extraction des données}


\subsubsection{5.3.1.1 Fichier JSON}

Le problème principal que nous avons rencontré lors de l'extraction des données est la gestion des fichiers JSON.
En effet, ces fichiers sont structurés de manière complexe et imbriquée, ce qui rend leur traitement plus difficile que les fichiers CSV.
Pour résoudre ce problème, nous avons dû développer des scripts Python spécifiques pour extraire les données pertinentes et les stocker dans des tables avant de les intégrer dans la base de données finale.
Lors de cette phase, nous n'avons pas "perdu" de données, mais nous avons dû faire des choix sur les informations à extraire et à stocker en fonction de leur pertinence pour les analyses futures.

\subsubsection{5.3.1.2 Fichier CSV}

Le problème rencontré avec le fichier CSV etait sa non conformité avec le schéma de la base de données.
En effet, ce fichier qui contient 4 colonnes, contient des lignes qui ont plus de 4 colonnes.
Cela est dû à la présence de virgules dans les champs textuels qui n'ont pas été correctement échappées et donc qui ont été interprétées comme des séparateurs de colonnes.
On se retrouve donc avec des lignes que l'on doit volontairement ignorer car elles ne sont pas exploitables.
Lors de cette partie, nous avons perdu 5.23% des données.
Le nombre total de lignes du fichier CSV est de 1375578 et le nombre de lignes ignorées est de 58243 soit 1317335 lignes qui ont été insérées dans la base de données.
Ce nombre de lignes est suffisant pour effectuer des analyses pertinentes et significatives, par contraite de temps, nous avons choisi de ne pas traiter ces lignes érronées.


\subsection{5.3.2 Problèmes rencontrés lors de la transformation des données}

Lors de la transformation des données, notre problème principal à été de nettoyer et de normaliser les données brutes pour les rendre cohérentes et exploitables pour l’analyse.
Cela implique de comprendre nos données et leurs caractéristiques.
Pour cela, nous avons exploré nos données en utilisant des requêtes SQL sur notre base de données.

\subsubsection{5.3.2.1 Fichier CSV}

Pour le fichier "yelp_academic_dataset_tip.csv", nous avons détecté des valeurs érronées dans toutes les colonnes.

Voici les elements qui nous ont permis de détecter ces valeurs érronées :

\begin{itemize}
\item Colonnes "user\_id" et "business\_id" : Un user\_id ou un business\_id ne peuvent pas être vide, ils ont une taille de 22 caractères et ne contiennent aucun espace.
Pour filtrer les lignes incorrectes, nous avons utilisé la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(business_id) != 22
OR business_id GLOB '* *'
OR length(user_id) != 22
OR user_id GLOB '* *'
OR user_id IS NULL
OR business_id IS NULL;
\end{verbatim}

On supprime donc les lignes qui ne respectent pas ces conditions et on est sûr que les données restantes sont cohérentes.

\item Colonne "compliment_count" : Cette colonne doit contenir des entiers positifs entre 0 et 10. On supprime donc les lignes qui ne respectent pas ces conditions via la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE compliment_count GLOB '*[^0-9]*'
OR compliment_count IS NULL;
\end{verbatim}

\item Colonne "date" : Cette colonne doit contenir des dates au format DATETIME. On supprime donc les lignes qui ne respectent pas ces conditions avec la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(date) != 19
OR date NOT GLOB '????-??-?? ??:??:??'
OR date IS NULL;
\end{verbatim}


Voici le tableau qui résume les données avant et après le nettoyage :

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Etape \& Nombre de lignes \\& Pourcentage de lignes conservées \\
\hline
Fichier brut \& 1375578 \& 100\% \\
\hline
Insertion \& 1317335 \& 95,76592531\% \\
\hline
Nettoyage \& 1315972 \& 95,66683968\% \\
\hline
\end{tabular}
\end{center}

Nous trouvons que le pourcentage de lignes conservées entre le fichier brut et le fichier nettoyé reste suffisamment élevé pour effectuer des analyses pertinentes et significatives.


\subsubsection{5.3.2.2 Fichier JSON}

Pour les fichiers "yelp_academic_dataset_business.json" et "yelp_academic_dataset_checkin.json", nous n'avons pas trouvé de problèmes particuliers, les données étaient déjà bien structurées et cohérentes.
Nous avons pu détecter cela en faisant des comparatifs avant et après le nettoyage des données.

Voici un tableau qui résume les données avant et après le nettoyage :

Nous arrivons donc à extraire et à nettoyer les données de ces fichiers sans aucune perte de données. Toutefois, des champs dans certaines tables sont vides, par exemple, pour un commerce,
l'adresse peut être vide, nous avons juger que supprimer des lignes pour ces cas n'était pas pertinent.

\textbf{
}

\subsection{5.3.3 Problèmes rencontrés lors du chargement des données}

Lors du chargement des données, nous n'avons pas rencontré de problèmes majeurs.
La commande COPY de PostgreSQL s'est avérée très efficace pour charger les données en blocs et réduire considérablement le temps de chargement.
Ce processus à une durée en moyenne de moins de 30 secondes pour l'ensemble des opération de chargement des données.
Par manque de temps, nous n'avons pas fais de comparatif entre la méthode COPY et une insertion ligne par ligne, mais nous sommes convaincus que la méthode COPY est la plus efficace pour charger de grandes quantités de données.
Après le chargement des données, nous n'avons pas detecté d'anomalies sur nos données dans le SGBD PostgreSQL (kafka.iem),


\chapter*{Analyse et interrogation des données}

\item 5.1 Construction des Data Marts


\item 5.2 Requêtes et agrégations
Requêtes SQL exécutées, cubes analytiques, et résultats obtenus.


\item 5.3 Analyse des performances
Mesure des temps d’exécution et suggestions d’amélioration.


\chapter*{Présentation des résultats}


\item 6.1 Tableaux de bord et visualisations
Utilisation de Metabase pour créer des dashboards et visualisations.


\item 6.2 Synthèse des résultats analytiques
Interprétation des résultats, insights pour les utilisateurs finaux.



\chapter*{Documentation technique}

\item 8.1 Instructions pour la compilation et l’exécution du code
Procédure détaillée.


\item 8.2 Liste des dépendances et configurations
Bibliothèques utilisées, versions des outils, etc.



\chapter*{Bilan}


\chapter*{Annexes}
