%! Author = maximecolliat
%! Date = 09/01/2025

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

\textbf{\begin{enumerate}
            \item \chapter*{Résumé}

\end{enumerate}

\begin{enumerate}
    \item \chapter*{Introduction}

\end{enumerate}

\section*{2.1 Contexte}
Dans le cadre du module d’Informatique Décisionnelle du Master 2 BDIA, ce projet propose d’explorer et d’appliquer les concepts fondamentaux de la création et de l’exploitation d’un data warehouse, un élément clé dans les environnements décisionnels modernes. L’importance croissante des données massives a profondément transformé les processus de prise de décision des entreprises, les poussant à s’appuyer sur des analyses précises et des outils avancés pour rester compétitifs.

Dans ce contexte, ce projet simule des problématiques réelles rencontrées par des analystes, des décideurs ou des responsables d’entreprise souhaitant exploiter des données volumineuses pour identifier des tendances, formuler des prévisions ou optimiser leurs opérations. Les données proviennent de l’application Yelp, une plateforme reconnue qui regroupe des millions d’avis d’utilisateurs sur divers commerces. 

Ce projet couvre l’ensemble des étapes essentielles du cycle de vie d’un système décisionnel : modélisation, intégration, analyse et présentation des données. 


\section*{2.2 Objectifs}
L’objectif principal de ce projet est de construire un système décisionnel complet et fonctionnel à partir d’un jeu de données volumineux et hétérogène fourni par l’application Yelp. Cela implique :


\begin{enumerate}
\item La modélisation d’un data warehouse adapté aux besoins analytiques identifiés, en choisissant entre des approches méthodologiques reconnues (Kimball ou Inmon).


\end{enumerate}

\begin{enumerate}
\item La conception et la mise en œuvre d’un processus ETL pour intégrer des données provenant de multiples formats et sources, tout en assurant leur cohérence et leur disponibilité pour l’analyse.


\end{enumerate}

\begin{enumerate}
\item L’élaboration de requêtes analytiques pertinentes, permettant d'explorer les données et de répondre à des questions stratégiques, telles que l’identification des commerces les mieux évalués ou la segmentation des utilisateurs selon leurs préférences.


\end{enumerate}

\begin{enumerate}
\item La création de tableaux de bord interactifs et intuitifs, permettant de synthétiser les résultats et de les communiquer efficacement à un public non technique.


\end{enumerate}

Pour la conception de notre data warehouse, nommé “Argon” en référence à l’élément chimique réputé pour sa stabilité et sa résistance, nous avons visé une architecture robuste et performante. Ce nom symbolise un système capable de centraliser et protéger les données tout en offrant une grande efficacité pour les analyses. Inspirée des propriétés de l’argon, notre conception met l’accent sur la simplicité, l’optimisation des performances, et l’isolation des composants pour garantir flexibilité et résilience face aux évolutions des données. 


Nos analyses se concentrent sur les commerces, en mettant l'accent sur les facteurs clés qui influencent leur implantation et leur succès dans différents lieux, en exploitant les données géographiques, les avis utilisateurs et les caractéristiques spécifiques des établissements.


\begin{enumerate}
\item \chapter*{Analyse du sujet et des données}

\end{enumerate}

\section*{3.1 Description des données disponibles

}
Les données exploitées dans ce projet proviennent de la plateforme Yelp et sont organisées sous différents formats et sources. Elles offrent une vue détaillée des commerces, des utilisateurs et de leurs interactions. Voici une présentation des principales sources de données :


\subsection*{3.1.1. Fichiers CSV et JSON}

Fichier CSV "tip" : Contient des avis courts et succincts donnés par les utilisateurs. Ces tips incluent des informations comme le contenu textuel, l’identifiant de l’utilisateur, la date et le commerce concerné.


Fichiers JSON :

\begin{itemize}
\item "checkin" : Liste des visites (checkins) effectuées dans les commerces. Ces données incluent des horodatages précis, permettant de suivre la fréquentation des établissements sur une période donnée.


\item "business" : Informations détaillées sur les commerces, telles que leur localisation, leurs catégories, leurs horaires d'ouverture et des attributs supplémentaires (présence d’un parking, options alimentaires, etc.).


\end{itemize}

\subsection*{3.1.2. Tables dans la base PostgreSQL}
La base de données PostgreSQL contient plusieurs tables liées aux utilisateurs et aux avis :

\begin{itemize}
\item Table "yelp.user" : Fournit des informations sur les utilisateurs, telles que leur identifiant unique et les statistiques associées à leur activité (nombre de contributions, réactions reçues, etc.).


\item Table "yelp.friend" : Décrit les relations d’amitié entre utilisateurs, permettant de modéliser les réseaux sociaux et leurs impacts potentiels sur les évaluations.


\item Table "yelp.elite" : Indique les années où les utilisateurs ont obtenu le statut d’élite.


\item Table "yelp.review" : Contient les évaluations des utilisateurs avec des notes, des commentaires textuels et des métadonnées supplémentaires, telles que la date et l’identifiant du commerce évalué.


\end{itemize}
Voici le diagramme relationnel de ce schéma de base de données :



\section*{3.2 Analyse des besoins}

Identification des questions clés à résoudre (exemples fournis dans le document).


\section*{3.3 Définition des objectifs analytiques}

Objectifs à atteindre et cas d’usage envisagés.


\begin{enumerate}
\item \chapter*{Conception du Data Warehouse}

\end{enumerate}

\section*{4.1 Approche méthodologique}
Kimball


\section*{4.2 Spécifications des schémas}

Schéma étoile, flocon ou constellation. Présentation des tables de faits et des dimensions.


\section*{4.3 Justifications des choix techniques

}
Arguments pour les schémas choisis en fonction des objectifs analytiques.


\begin{enumerate}
\item \chapter*{Intégration des données}

\end{enumerate}

\section*{5.1 Architecture de l’ETL}

Un ETL est un processus d’extraction, de transformation et de chargement des données dans un entrepôt de données. Il est essentiel pour garantir la qualité et la cohérence des données.

\subsection{5.1.1 Extraction des données}

La partie extraction consiste à récupérer les données brutes à partir de différentes sources, telles que des fichiers CSV, des bases de données, des API, etc. Ces données sont ensuite stockées dans une zone de transit pour être traitées par la suite.
Nous avons choisi d’utiliser le langage Python avec différentes librairies comme Pandas pour implémenter notre ETL.
Nous avons opté pour cette solution en raison de sa simplicité et l'aisance que nous en avons grâce aux séances de l'UE Machine Learning et Deep Learning où nous avons utilisé Pandas pour manipuler des données dans des fichiers CSV et JSON.
Via des scripts python qui ont été fait pour chaque fichier, notre but est de parser chaque fichier et d'insérer les données dans une base de données Sqlite.

\subsection{5.1.2 Transformation des données}

La transformation des données est une étape cruciale pour nettoyer, normaliser et enrichir les données brutes. Elle permet de les rendre cohérentes et exploitables pour l’analyse. Cette phase inclut des opérations telles que le filtrage, le tri, la jointure, l’agrégation, la déduplication, etc.
Cette transformation est réalisée en utilisant des requêtes SQL pour manipuler les données dans la base de données Sqlite.
Le choix d'opter pour une base de données SQLite est idéal pour ce type de tâche car il est rapide et performant pour des volumes modérés de données,tout en n'exigeant aucune infrastructure lourde.
Il offre également une gestion de la consistance des données avec des transactions ACID, assurant une intégrité des données lors des transformations.
Son gros avantage par rapport aux autres SGBD est qu'il est stocké dans un fichier, ce qui le rend facilement transportable et utilisable sur n'importe quel système d'exploitation.
Ainsi, si pour une raison, nous voulons insérer les données dans un autre SGBD, nous pourrons le faire facilement.

Via des fichiers SQL, nous avons implémenté les requêtes nécessaires pour transformer les données brutes en données exploitables pour l'analyse.

\subsection{5.1.3 Chargement des données}

La dernière étape de l’ETL consiste à charger les données transformées dans le data warehouse. Ces données sont stockées dans des tables spécifiques, prêtes à être
analysées par des outils de business intelligence ou des applications d’analyse de données.

Les données transformées sont chargées dans un SGBD Oracle, qui est un système de gestion de base de données relationnelle très performant et robuste. Il offre des fonctionnalités avancées pour l’analyse de données et la gestion des requêtes complexes.


\section*{5.2 Processus d’intégration}


\section{5.3 Problèmes rencontrés et solutions apportées}

\subsection{5.3.1 Problèmes rencontrés lors de l'extraction des données}

Lors de l'extraction des données, notre problème principal à été de parser chaque fichier et d'insérer les données dans une base de données Sqlite.

\subsubsection{5.3.1.1 Fichier JSON}

Le problème principal que nous avons rencontré lors de l'extraction des données est la gestion des fichiers JSON.
En effet, ces fichiers sont structurés de manière complexe et imbriquée, ce qui rend leur traitement plus difficile que les fichiers CSV.
Pour résoudre ce problème, nous avons dû développer des scripts Python spécifiques pour extraire les données pertinentes et les stocker dans des tables avant de les intégrer dans la base de données finale.
Lors de cette phase, nous n'avons pas "perdu" de données, mais nous avons dû faire des choix sur les informations à extraire et à stocker en fonction de leur pertinence pour les analyses futures.

\subsubsection{5.3.1.2 Fichier CSV}

Le problème rencontré avec le fichier CSV etait sa non conformité avec le schéma de la base de données.
En effet, ce fichier qui contient 4 colonnes, contient des lignes qui ont plus de 4 colonnes.
Cela est dû à la présence de virgules dans les champs textuels qui n'ont pas été correctement échappées et donc qui ont été interprétées comme des séparateurs de colonnes.
On se retrouve donc avec des lignes que l'on doit volontairement ignorer car elles ne sont pas exploitables.
Lors de cette partie, nous avons perdu 5.23% des données.
Le nombre total de lignes du fichier CSV est de 1375578 et le nombre de lignes ignorées est de 58243 soit 1317335 lignes qui ont été insérées dans la base de données.
Ce nombre de lignes est suffisant pour effectuer des analyses pertinentes et significatives, par contraite de temps, nous avons choisi de ne pas traiter ces lignes érronées.


\subsection{5.3.2 Problèmes rencontrés lors de la transformation des données}

Lors de la transformation des données, notre problème principal à été de nettoyer et de normaliser les données brutes pour les rendre cohérentes et exploitables pour l’analyse.
Cela implique de comprendre nos données et leurs caractéristiques.
Pour cela, nous avons exploré nos données en utilisant des requêtes SQL sur notre base de données Sqlite.

\subsubsection{5.3.2.1 Fichier CSV}

Pour le fichier "yelp_academic_dataset_tip.csv", nous avons détecté des valeurs érronées dans toutes les colonnes.

Voici les elements qui nous ont permis de détecter ces valeurs érronées :

\begin{itemize}
\item Colonnes "user\_id" et "business\_id" : Un user\_id ou un business\_id ne peuvent pas être vide, ils ont une taille de 22 caractères et ne contiennent aucun espace.
Pour filtrer les lignes incorrectes, nous avons utilisé la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(business_id) != 22
OR business_id GLOB '* *'
OR length(user_id) != 22
OR user_id GLOB '* *';
\end{verbatim}

On supprime donc les lignes qui ne respectent pas ces conditions et on est sûr que les données restantes sont cohérentes.

\item Colonne "compliment_count" : Cette colonne doit contenir des entiers positifs entre 0 et 10. On supprime donc les lignes qui ne respectent pas ces conditions via la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE compliment_count GLOB '*[^0-9]*';
\end{verbatim}

\item Colonne "date" : Cette colonne doit contenir des dates au format DATETIME. On supprime donc les lignes qui ne respectent pas ces conditions avec la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(date) != 19
OR date NOT GLOB '????-??-?? ??:??:??';;
\end{verbatim}


Voici le tableau qui résume les données avant et après le nettoyage :

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Etape & Nombre de lignes & Pourcentage de lignes conservées \\
\hline
Fichier brut & 1375578 & 100\% \\
\hline
Insertion & 1317335 & 95,76592531\% \\
\hline
Nettoyage & 1315972 & 95,66683968\% \\
\hline
\end{tabular}
\end{center}


\subsubsection{5.3.2.2 Fichier JSON}

Pour les fichiers "yelp_academic_dataset_business.json" et "yelp_academic_dataset_checkin.json", nous n'avons pas trouvé de problèmes particuliers, les données étaient déjà bien structurées et cohérentes.


\begin{enumerate}
\item \chapter*{Analyse et interrogation des données}

\end{enumerate}


\begin{itemize}
\item 5.1 Construction des Data Marts
Création des sous-ensembles de données spécifiques à des analyses.


\item 5.2 Requêtes et agrégations
Requêtes SQL exécutées, cubes analytiques, et résultats obtenus.


\item 5.3 Analyse des performances
Mesure des temps d’exécution et suggestions d’amélioration.


\end{itemize}

\begin{enumerate}
\item \chapter*{Présentation des résultats}

\end{enumerate}

\begin{itemize}
\item 6.1 Tableaux de bord et visualisations
Utilisation de Metabase pour créer des dashboards et visualisations.


\item 6.2 Synthèse des résultats analytiques
Interprétation des résultats, insights pour les utilisateurs finaux.


\end{itemize}

\begin{enumerate}
\item \chapter*{Documentation technique}

\end{enumerate}
\begin{itemize}
\item 8.1 Instructions pour la compilation et l’exécution du code
Procédure détaillée.


\item 8.2 Liste des dépendances et configurations
Bibliothèques utilisées, versions des outils, etc.


\end{itemize}

\begin{enumerate}
\item \chapter*{Bilan}

\end{enumerate}

\begin{itemize}
\item Résumé des contributions principales du projet.


\item Perspectives pour des développements futurs.


\end{itemize}

\begin{enumerate}
\item \chapter*{Annexes}

\end{enumerate}
}