\section*{Résumé}


\section*{Introduction}

\subsection{Contexte}\label{subsec:contexte}
Dans le cadre du module d’Informatique Décisionnelle du Master 2 BDIA, ce projet propose d’explorer et d’appliquer les concepts fondamentaux de la création et de l’exploitation d’un data warehouse, un élément clé dans les environnements décisionnels modernes. L’importance croissante des données massives a profondément transformé les processus de prise de décision des entreprises, les poussant à s’appuyer sur des analyses précises et des outils avancés pour rester compétitifs.

Dans ce contexte, ce projet simule des problématiques réelles rencontrées par des analystes, des décideurs ou des responsables d’entreprise souhaitant exploiter des données volumineuses pour identifier des tendances, formuler des prévisions ou optimiser leurs opérations. Les données proviennent de l’application Yelp, une plateforme reconnue qui regroupe des millions d’avis d’utilisateurs sur divers commerces. 

Ce projet couvre l’ensemble des étapes essentielles du cycle de vie d’un système décisionnel : modélisation, intégration, analyse et présentation des données. 


\section*{2.2 Objectifs}
L’objectif principal de ce projet est de construire un système décisionnel complet et fonctionnel à partir d’un jeu de données volumineux et hétérogène fourni par l’application Yelp. Cela implique :


\begin{enumerate}
\item La modélisation d’un data warehouse adapté aux besoins analytiques identifiés, en choisissant entre des approches méthodologiques reconnues (Kimball ou Inmon).
\item La conception et la mise en œuvre d’un processus ETL pour intégrer des données provenant de multiples formats et sources, tout en assurant leur cohérence et leur disponibilité pour l’analyse.
\item L’élaboration de requêtes analytiques pertinentes, permettant d'explorer les données et de répondre à des questions stratégiques, telles que l’identification des commerces les mieux évalués ou la segmentation des utilisateurs selon leurs préférences.
\item La création de tableaux de bord interactifs et intuitifs, permettant de synthétiser les résultats et de les communiquer efficacement à un public non technique.
\end{enumerate}

Pour la conception de notre data warehouse, nommé “Argon” en référence à l’élément chimique réputé pour sa stabilité et sa résistance, nous avons visé une architecture robuste et performante. Ce nom symbolise un système capable de centraliser et protéger les données tout en offrant une grande efficacité pour les analyses. Inspirée des propriétés de l’argon, notre conception met l’accent sur la simplicité, l’optimisation des performances, et l’isolation des composants pour garantir flexibilité et résilience face aux évolutions des données. 


Nos analyses se concentrent sur les commerces, en mettant l'accent sur les facteurs clés qui influencent leur implantation et leur succès dans différents lieux, en exploitant les données géographiques, les avis utilisateurs et les caractéristiques spécifiques des établissements.


\chapter*{Analyse du sujet et des données}


\section*{3.1 Description des données disponibles}
Les données exploitées dans ce projet proviennent de la plateforme Yelp et sont organisées sous différents formats et sources. Elles offrent une vue détaillée des commerces, des utilisateurs et de leurs interactions. Voici une présentation des principales sources de données :


\subsection*{3.1.1. Fichiers CSV et JSON}

Fichier CSV "tip" : Contient des avis courts et succincts donnés par les utilisateurs. Ces tips incluent des informations comme le contenu textuel, l’identifiant de l’utilisateur, la date et le commerce concerné.


Fichiers JSON :

\begin{itemize}
\item "checkin" : Liste des visites (checkins) effectuées dans les commerces. Ces données incluent des horodatages précis, permettant de suivre la fréquentation des établissements sur une période donnée.


\item "business" : Informations détaillées sur les commerces, telles que leur localisation, leurs catégories, leurs horaires d'ouverture et des attributs supplémentaires (présence d’un parking, options alimentaires, etc.).


\end{itemize}

\subsection*{3.1.2. Tables dans la base PostgreSQL}
La base de données PostgreSQL contient plusieurs tables liées aux utilisateurs et aux avis :

\begin{itemize}
\item Table "yelp.user" : Fournit des informations sur les utilisateurs, telles que leur identifiant unique et les statistiques associées à leur activité (nombre de contributions, réactions reçues, etc.).


\item Table "yelp.friend" : Décrit les relations d’amitié entre utilisateurs, permettant de modéliser les réseaux sociaux et leurs impacts potentiels sur les évaluations.


\item Table "yelp.elite" : Indique les années où les utilisateurs ont obtenu le statut d’élite.


\item Table "yelp.review" : Contient les évaluations des utilisateurs avec des notes, des commentaires textuels et des métadonnées supplémentaires, telles que la date et l’identifiant du commerce évalué.


\end{itemize}

Voici le diagramme relationnel de ce schéma de base de données :

\section*{3.2 Analyse des besoins}

Pour répondre aux besoins des utilisateurs finaux, nous avons identifié plusieurs cas d’usage et scénarios d’analyse qui nécessitent des requêtes complexes et des agrégations de données.
Voici quelques exemples de questions auxquelles nous souhaitons répondre :

\begin{itemize}
\item Quels sont les 20 villes avec le plus de commerces ?
\item Quels sont les heures d'ouverture les plus communes sur une semaine ?
\item Quels sont les heures d'ouverture les plus communes par jour de la semaine ?
\item Combien de business sont ouverts 24h/24 7j/7 ?
\item Quels sont les attributs les plus courants des commerces ?
\end{itemize}

Ces questions nous permettront de mieux comprendre les tendances et les caractéristiques des commerces, ainsi que les préférences des utilisateurs. Elles guideront la conception de notre data warehouse et l’élaboration des requêtes analytiques.


\section*{3.3 Définition des objectifs analytiques}

Sur la base des besoins identifiés, nous avons défini les objectifs analytiques suivants :

\begin{itemize}
\item Identifier les villes les plus attractives pour les commerces, en fonction de leur densité et de leur popularité.
\item Analyser les horaires d’ouverture les plus courants des commerces, pour optimiser leur disponibilité et leur visibilité.
\item Étudier les attributs les plus fréquents des commerces, pour identifier les services les plus recherchés par les utilisateurs.
\end{itemize}

Nous pensons que ces analyses permettront de mieux comprendre les tendances et les préférences des utilisateurs, ainsi que les caractéristiques des commerces les plus populaires. Ces informations peuvent être utiles pour les propriétaires d’établissements, les gestionnaires de franchises ou les analystes de marché qui souhaitent optimiser leurs stratégies commerciales et leur présence en ligne.
Par exemple, en identifiant les villes les plus attractives, les commerçants peuvent cibler leurs efforts de marketing et d’expansion pour maximiser leur visibilité et leur rentabilité. De même, en analysant les horaires d’ouverture les plus courants, ils peuvent ajuster leurs plages horaires pour répondre aux besoins de leur clientèle et augmenter leur fréquentation.

\chapter*{Conception du Data Warehouse}


\section*{4.1 Approche méthodologique}

Pour la conception de notre data warehouse, nous avons choisi d’adopter l'approche de Kimball, qui privilégie une modélisation orientée métier et centrée sur les besoins des utilisateurs finaux.
Cette approche se distingue par sa simplicité, sa flexibilité et sa capacité à s’adapter aux évolutions des données et des besoins analytiques.
Elle repose sur des concepts clés tels que les faits, les dimensions, les grains de granularité et les schémas en étoile ou en flocon.

Nous avons opté pour un schéma en étoile pour notre data warehouse, en raison de sa simplicité et de sa facilité d’utilisation pour les analyses multidimensionnelles.
Ce schéma est composé d’une table de faits centrale, entourée de tables de dimensions qui décrivent les contextes et les attributs des données.
Cette structure en étoile permet de simplifier les requêtes et d’optimiser les performances des analyses.
L'approche de Inmon, qui privilégie une modélisation normalisée et une intégration des données en amont, aurait pu être une alternative, mais nous avons préféré la flexibilité et la simplicité offertes par le schéma en étoile.
Notre conception prend en compte une evolution future des besoins et des données, en permettant l’ajout de nouvelles dimensions ou de nouveaux faits sans impacter l’existant.

\section*{4.2 Spécifications des schémas}

Voici notre schéma en étoile pour le data warehouse “Argon” :


\section*{4.3 Justifications des choix techniques}


\chapter*{Intégration des données}


\section*{5.1 Architecture de l’ETL}

Un ETL est un processus d’extraction, de transformation et de chargement des données dans un entrepôt de données. Il est essentiel pour garantir la qualité et la cohérence des données.

Voici le schéma général de notre processus ETL :


\subsection{5.1.1 Extraction des données}


Nous avons choisi d’utiliser le langage Python avec différentes librairies comme Pandas pour implémenter notre ETL. Nous avons opté pour cette solution en raison de sa simplicité et l'aisance que nous avons grâce aux séances de l'UE Machine Learning et Deep Learning, où nous avons utilisé Pandas pour manipuler des données dans des fichiers CSV et JSON. 

Via des scripts Python développés pour chaque fichier, nous avons extrait les données et les avons stockées dans une base de données SQLite.
Ces scripts sont disponibles dans le dossier "/script" du projet.


Le choix de SQLite s’est imposé pour plusieurs raisons. Tout d’abord, il s’agit d’une solution légère et intégrée qui ne nécessite aucune configuration complexe ni serveur dédié,
ce qui simplifie le déploiement et la maintenance. Ensuite, SQLite est entièrement autonome et stocke les données dans un seul fichier, ce qui facilite grandement le transport et le
partage de la base de données. Enfin, sa compatibilité avec la plupart des systèmes d’exploitation et son support natif dans Python en font une option pratique et efficace pour
gérer les données du projet.
Ici, SQLite est utilisé comme un entrepôt de données temporaire pour stocker les données brutes avant de les transformer et de les charger dans le data warehouse final.
Nous voulons que les données soient stockées dans un format tabulaire pour faciliter leur manipulation et leur analyse ultérieure. SQLite offre une solution simple et efficace pour
stocker des données structurées dans des tables relationnelles, ce qui en fait un choix idéal pour cette tâche.


Voici un schéma de l'extraction des données et le schéma relationnel qui en découle :

\subsection{5.1.2 Transformation des données}

La transformation des données est une étape cruciale pour nettoyer, normaliser et enrichir les données brutes.
Elle permet de les rendre cohérentes et exploitables pour l’analyse. Cette phase inclut des opérations telles que le filtrage, le tri, la jointure, l’agrégation, la déduplication, etc.
À travers des fichiers SQL, nous avons implémenté les requêtes nécessaires pour transformer les données brutes en un format directement exploitable pour l’analyse.

\subsection{5.1.3 Chargement des données}

La dernière étape de l’ETL consiste à charger les données transformées dans le data warehouse. Ces données sont stockées dans des tables spécifiques, prêtes à être
analysées par des outils de business intelligence ou des applications d’analyse de données.

Les données transformées sont chargées dans un SGBD Oracle, qui est un système de gestion de
base de données relationnelle très performant et robuste. Il offre des fonctionnalités avancées pour l’analyse de données et la gestion des requêtes complexes.


\section*{5.2 Processus d’intégration}

Le processus d’intégration consiste à combiner les données provenant de différentes sources et à les stocker dans un entrepôt de données centralisé.
Cela permet de disposer d’une vue unifiée et cohérente des informations, facilitant leur analyse et leur exploitation.


\section{5.3 Problèmes rencontrés et solutions apportées}

Dans cette section, nous décrivons les problèmes rencontrés lors de l’extraction et de la transformation des données, ainsi que les solutions mises en place pour les résoudre.

\subsection{5.3.1 Problèmes rencontrés lors de l'extraction des données}


\subsubsection{5.3.1.1 Fichier JSON}

Le problème principal que nous avons rencontré lors de l'extraction des données est la gestion des fichiers JSON.
En effet, ces fichiers sont structurés de manière complexe et imbriquée, ce qui rend leur traitement plus difficile que les fichiers CSV.
Pour résoudre ce problème, nous avons dû développer des scripts Python spécifiques pour extraire les données pertinentes et les stocker dans des tables avant de les intégrer dans la base de données finale.
Lors de cette phase, nous n'avons pas "perdu" de données, mais nous avons dû faire des choix sur les informations à extraire et à stocker en fonction de leur pertinence pour les analyses futures.

\subsubsection{5.3.1.2 Fichier CSV}

Le problème rencontré avec le fichier CSV etait sa non conformité avec le schéma de la base de données.
En effet, ce fichier qui contient 4 colonnes, contient des lignes qui ont plus de 4 colonnes.
Cela est dû à la présence de virgules dans les champs textuels qui n'ont pas été correctement échappées et donc qui ont été interprétées comme des séparateurs de colonnes.
On se retrouve donc avec des lignes que l'on doit volontairement ignorer car elles ne sont pas exploitables.
Lors de cette partie, nous avons perdu 5.23% des données.
Le nombre total de lignes du fichier CSV est de 1375578 et le nombre de lignes ignorées est de 58243 soit 1317335 lignes qui ont été insérées dans la base de données.
Ce nombre de lignes est suffisant pour effectuer des analyses pertinentes et significatives, par contraite de temps, nous avons choisi de ne pas traiter ces lignes érronées.


\subsection{5.3.2 Problèmes rencontrés lors de la transformation des données}

Lors de la transformation des données, notre problème principal à été de nettoyer et de normaliser les données brutes pour les rendre cohérentes et exploitables pour l’analyse.
Cela implique de comprendre nos données et leurs caractéristiques.
Pour cela, nous avons exploré nos données en utilisant des requêtes SQL sur notre base de données.

\subsubsection{5.3.2.1 Fichier CSV}

Pour le fichier "yelp_academic_dataset_tip.csv", nous avons détecté des valeurs érronées dans toutes les colonnes.

Voici les elements qui nous ont permis de détecter ces valeurs érronées :

\begin{itemize}
\item Colonnes "user\_id" et "business\_id" : Un user\_id ou un business\_id ne peuvent pas être vide, ils ont une taille de 22 caractères et ne contiennent aucun espace.
Pour filtrer les lignes incorrectes, nous avons utilisé la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(business_id) != 22
OR business_id GLOB '* *'
OR length(user_id) != 22
OR user_id GLOB '* *'
OR user_id IS NULL
OR business_id IS NULL;
\end{verbatim}

On supprime donc les lignes qui ne respectent pas ces conditions et on est sûr que les données restantes sont cohérentes.

\item Colonne "compliment_count" : Cette colonne doit contenir des entiers positifs entre 0 et 10. On supprime donc les lignes qui ne respectent pas ces conditions via la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE compliment_count GLOB '*[^0-9]*'
OR compliment_count IS NULL;
\end{verbatim}

\item Colonne "date" : Cette colonne doit contenir des dates au format DATETIME. On supprime donc les lignes qui ne respectent pas ces conditions avec la requête suivante :

\begin{verbatim}
DELETE
FROM tips
WHERE length(date) != 19
OR date NOT GLOB '????-??-?? ??:??:??'
OR date IS NULL;
\end{verbatim}


Voici le tableau qui résume les données avant et après le nettoyage :

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Etape \& Nombre de lignes \\& Pourcentage de lignes conservées \\
\hline
Fichier brut \& 1375578 \& 100\% \\
\hline
Insertion \& 1317335 \& 95,76592531\% \\
\hline
Nettoyage \& 1315972 \& 95,66683968\% \\
\hline
\end{tabular}
\end{center}

Nous trouvons que le pourcentage de lignes conservées entre le fichier brut et le fichier nettoyé reste suffisamment élevé pour effectuer des analyses pertinentes et significatives.


\subsubsection{5.3.2.2 Fichier JSON}

Pour les fichiers "yelp_academic_dataset_business.json" et "yelp_academic_dataset_checkin.json", nous n'avons pas trouvé de problèmes particuliers, les données étaient déjà bien structurées et cohérentes.


\chapter*{Analyse et interrogation des données}

\item 5.1 Construction des Data Marts


\item 5.2 Requêtes et agrégations
Requêtes SQL exécutées, cubes analytiques, et résultats obtenus.


\item 5.3 Analyse des performances
Mesure des temps d’exécution et suggestions d’amélioration.





\chapter*{Présentation des résultats}


\item 6.1 Tableaux de bord et visualisations
Utilisation de Metabase pour créer des dashboards et visualisations.


\item 6.2 Synthèse des résultats analytiques
Interprétation des résultats, insights pour les utilisateurs finaux.



\chapter*{Documentation technique}

\item 8.1 Instructions pour la compilation et l’exécution du code
Procédure détaillée.


\item 8.2 Liste des dépendances et configurations
Bibliothèques utilisées, versions des outils, etc.



\chapter*{Bilan}


\chapter*{Annexes}
